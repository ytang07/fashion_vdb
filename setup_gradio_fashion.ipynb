{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Gonna just use the Taylor Swift Images in this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ZILLIZ_URI = os.getenv(\"ZILLIZ_URI\")\n",
    "ZILLIZ_TOKEN = os.getenv(\"ZILLIZ_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility, connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(uri=ZILLIZ_URI, token=ZILLIZ_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yujiantang/Documents/workspace/fashionai/fai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, tensor\n",
    "from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Resize\n",
    "import torchvision.transforms as T\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.transforms.functional import crop\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/yujiantang/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/Users/yujiantang/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "/Users/yujiantang/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "/Users/yujiantang/Documents/workspace/fashionai/fai/lib/python3.10/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/Users/yujiantang/Documents/workspace/fashionai/fai/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:101: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# run this before importing the resnet50 model if you run into an SSL certificate URLError\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Load the embedding model with the last layer removed\n",
    "embeddings_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "embeddings_model = torch.nn.Sequential(*(list(embeddings_model.children())[:-1]))\n",
    "embeddings_model.eval()\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "segmentation_model = SegformerForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for image in glob.glob(\"./photos/Taylor_Swift/*.jpg\", recursive=True):\n",
    "    image_paths.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 2048\n",
    "BATCH_SIZE = 128\n",
    "COLLECTION_NAME = \"TSwizzleFashionComparison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, Collection, DataType\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='filepath', dtype=DataType.VARCHAR, max_length=200),\n",
    "    FieldSchema(name=\"name\", dtype=DataType.VARCHAR, max_length=200),\n",
    "    FieldSchema(name=\"seg_id\", dtype=DataType.INT64),\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, enable_dynamic_field=True)\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nlist\": 128},\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label= {\n",
    "    0: \"Background\",\n",
    "    1: \"Hat\",\n",
    "    2: \"Hair\",\n",
    "    3: \"Sunglasses\",\n",
    "    4: \"Upper-clothes\",\n",
    "    5: \"Skirt\",\n",
    "    6: \"Pants\",\n",
    "    7: \"Dress\",\n",
    "    8: \"Belt\",\n",
    "    9: \"Left-shoe\",\n",
    "    10: \"Right-shoe\",\n",
    "    11: \"Face\",\n",
    "    12: \"Left-leg\",\n",
    "    13: \"Right-leg\",\n",
    "    14: \"Left-arm\",\n",
    "    15: \"Right-arm\",\n",
    "    16: \"Bag\",\n",
    "    17: \"Scarf\"\n",
    "  }\n",
    "# want to extract keys: 1, 3, 4, 5, 6, 7, 8, 9, 10, 16, 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = [1, 3, 4, 5, 6, 7, 8, 9, 10, 16, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation(image):\n",
    "    inputs = extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = segmentation_model(**inputs)\n",
    "    logits = outputs.logits.cpu()\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "    return pred_seg \n",
    "\n",
    "# returns two lists masks (tensor) and obj_ids (int)\n",
    "# \"mattmdjaga/segformer_b2_clothes\" from hugging face\n",
    "def get_masks(segmentation):\n",
    "    obj_ids = torch.unique(segmentation)\n",
    "    obj_ids = obj_ids[1:]\n",
    "    wanted_ids = [x.item() for x in obj_ids if x in wanted]\n",
    "    # print(obj_ids)\n",
    "    # print(wanted_ids)\n",
    "    wanted_ids = torch.Tensor(wanted_ids)\n",
    "    # print(wanted_ids)\n",
    "    masks = segmentation == wanted_ids[:, None, None]\n",
    "    return masks, obj_ids\n",
    "\n",
    "def crop_images(masks, obj_ids, img):\n",
    "    boxes = masks_to_boxes(masks)\n",
    "    crop_boxes = []\n",
    "    for box in boxes:\n",
    "        crop_box = tensor([box[0], box[1], box[2]-box[0], box[3]-box[1]])\n",
    "        crop_boxes.append(crop_box)\n",
    "    \n",
    "    preprocess = T.Compose([\n",
    "        T.Resize(size=(256, 256)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    cropped_images = []\n",
    "    seg_ids = []\n",
    "    for i in range(len(crop_boxes)):\n",
    "        crop_box = crop_boxes[i]\n",
    "        cropped = crop(img, crop_box[1].item(), crop_box[0].item(), crop_box[3].item(), crop_box[2].item())\n",
    "        cropped_images.append(preprocess(cropped))\n",
    "        seg_ids.append(obj_ids[i].item())\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings_model(torch.stack(cropped_images)).squeeze().tolist()\n",
    "    return embeddings, boxes.tolist(), seg_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  3,  4,  6,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "[3, 4, 6, 9, 10, 16]\n",
      "tensor([ 3.,  4.,  6.,  9., 10., 16.])\n",
      "tensor([ 2,  4,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "[4, 6, 8, 9, 10, 16]\n",
      "tensor([ 4.,  6.,  8.,  9., 10., 16.])\n",
      "tensor([ 2,  4,  6,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "[4, 6, 9, 10, 16]\n",
      "tensor([ 4.,  6.,  9., 10., 16.])\n",
      "tensor([ 1,  2,  4,  6, 10, 11, 12, 13, 14, 15])\n",
      "[1, 4, 6, 10]\n",
      "tensor([ 1.,  4.,  6., 10.])\n",
      "tensor([ 2,  3,  4,  6,  9, 10, 11, 14, 15, 17])\n",
      "[3, 4, 6, 9, 10, 17]\n",
      "tensor([ 3.,  4.,  6.,  9., 10., 17.])\n",
      "tensor([ 2,  4,  6,  9, 10, 11, 14, 15])\n",
      "[4, 6, 9, 10]\n",
      "tensor([ 4.,  6.,  9., 10.])\n",
      "tensor([ 2,  3,  7, 11, 12, 13, 14, 15, 16])\n",
      "[3, 7, 16]\n",
      "tensor([ 3.,  7., 16.])\n",
      "tensor([ 1,  2,  4,  5, 11, 12, 13, 14, 15, 16])\n",
      "[1, 4, 5, 16]\n",
      "tensor([ 1.,  4.,  5., 16.])\n",
      "tensor([ 2,  4,  6,  7,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "[4, 6, 7, 9, 10, 16]\n",
      "tensor([ 4.,  6.,  7.,  9., 10., 16.])\n",
      "tensor([ 2,  4,  5,  6,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "[4, 5, 6, 9, 10, 16]\n",
      "tensor([ 4.,  5.,  6.,  9., 10., 16.])\n"
     ]
    }
   ],
   "source": [
    "for path in image_paths:\n",
    "    image = Image.open(path)\n",
    "    path_split = path.split(\"/\")\n",
    "    name = \" \".join(path_split[2].split(\"_\"))\n",
    "    segmentation = get_segmentation(image)\n",
    "    masks, ids = get_masks(segmentation)\n",
    "    embeddings, crop_corners, seg_ids = crop_images(masks, ids, image)\n",
    "    inserts = [{\"embedding\": embeddings[x],\n",
    "                \"seg_id\": seg_ids[x],\n",
    "                \"name\": name,\n",
    "                \"filepath\": path,\n",
    "                \"crop_corner\": crop_corners[x]} for x in range(len(embeddings))]\n",
    "    collection.insert(inserts)\n",
    "                \n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
